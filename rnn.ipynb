{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bliu/Documents/adversarial-networks/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from networks.rnn import CNNLSTM\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class CustomCIFAR(datasets.CIFAR10):\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "            #CHANGED from the torchvision implementation: pass the target into transform\n",
    "            img = self.transform((img, target))\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from networks.cnn import CNN\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Fast gradient sign method\n",
    "def fgsm(image, epsilon, data_grad):\n",
    "    \"\"\"Generate a perturbed image using the Fast Gradient Sign Method.\"\"\"\n",
    "    # eta = epsilon * sign(gradient of loss w.r.t input image)\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "\n",
    "def simple_test(test_loader, criterion, model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for inputs, labels in test_loader:\n",
    "        # for every batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = (torch.max(outputs, dim=1)[1] == labels).to(torch.float32).mean()\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        accuracies.append(accuracy.cpu().numpy())\n",
    "\n",
    "    loss, accuracy = np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def test_pre_post_fgsm(\n",
    "    model, test_dataset, test_loader, epsilon, criterion\n",
    "):\n",
    "    \"\"\"this function runs the model and compares the outputs with and without fgsm attack\"\"\"\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    for images, labels in test_loader:\n",
    "        # Send the data and label to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor\n",
    "        images.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(images)\n",
    "        # get the index of the max log-probability\n",
    "        init_pred = torch.max(output, dim=1)[1]\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect gradients\n",
    "        data_grad = images.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_images = fgsm(images, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_images)\n",
    "\n",
    "        final_pred = torch.max(output, dim=1)[\n",
    "            1\n",
    "        ]  # get the index of the max log-probability\n",
    "        correct_idx = final_pred == labels\n",
    "        correct += sum(correct_idx.to(torch.float32)).item()\n",
    "\n",
    "        # only get pred that was right buut now wrong\n",
    "        incorrect_idx = (final_pred != labels) & (init_pred == labels)\n",
    "\n",
    "        # saving examples of perturbed images for later visualization\n",
    "        if len(adv_examples) < 5:\n",
    "            # Save some adv examples for visualization later\n",
    "            # p is the single perturbed image, y is the correct label, initial and final and pre- and post-fgsm predictions\n",
    "            for initial, final, p, y in zip(\n",
    "                init_pred[incorrect_idx],\n",
    "                final_pred[incorrect_idx],\n",
    "                perturbed_images[incorrect_idx],\n",
    "                labels[incorrect_idx],\n",
    "            ):\n",
    "                adv_ex = p.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append((initial.item(), final.item(), y.item(), adv_ex))\n",
    "                # returned adv_examples is 1 x batchsize x 4, holding items: pre-fgsm pred, post-fgsm pred, ground truth, post-fgsm image\n",
    "\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if epsilon == 0:\n",
    "                for initial, final in zip(\n",
    "                    init_pred[correct_idx], final_pred[correct_idx]\n",
    "                ):\n",
    "                    adv_ex = perturbed_images.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append(\n",
    "                        (initial.item(), final.item(), final.item(), adv_ex)\n",
    "                    )\n",
    "\n",
    "    data_len = len(test_dataset)\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct / float(data_len)\n",
    "    print(\n",
    "        \"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(\n",
    "            epsilon, correct, data_len, final_acc\n",
    "        )\n",
    "    )\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download CIFAR10 dataset\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader= torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = CNNLSTM().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 50017/500000 [09:04<1:17:05, 97.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 84618/500000 [14:57<1:10:05, 98.77it/s] "
     ]
    }
   ],
   "source": [
    "# train the batch_size=1 model\n",
    "# Train the model\n",
    "progress = tqdm(total=len(train_loader)*EPOCHS, desc=\"Training\") # add a progress bar\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        model.train()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress.update(1)\n",
    "    \n",
    "    progress.write(f'Epoch [{epoch+1}/{10}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# saving model\n",
    "torch.save(model.state_dict(), \"models/cnn-lstm-batchsize1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = \"./models/cnn-lstm.pth\" \n",
    "base_model = CNNLSTM().to(device)\n",
    "base_model.load_state_dict(torch.load(BASE_MODEL_PATH))\n",
    "EPOCHS = 10\n",
    "EPSILONS =  [0.005, 0.01, 0.015, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "accuracies = {}\n",
    "losses = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGSMTransform:\n",
    "    \"\"\"Perform a fast gradient sign attack on an image.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.005, prob=1):\n",
    "        self.epsilon = epsilon\n",
    "        self.model = CNNLSTM().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.model.load_state_dict(torch.load(\"./models/cnn-lstm-batchsize1.pth\"))\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.prob = prob #probability of applying fgsm on an image\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def __call__(self,sample):\n",
    "        x, labels = sample\n",
    "        if np.random.rand() >= self.prob:\n",
    "            return x\n",
    "        #if we process a single image instead of batch, we need to add a fourth dimension as the batch dimension\n",
    "        x = x.unsqueeze(0)\n",
    "        labels = labels.unsqueeze(0)\n",
    "        x = x.to(self.device)\n",
    "        x.requires_grad = True\n",
    "\n",
    "        # forward, backward pass to calculate gradient\n",
    "        output = self.model(x)\n",
    "        loss = self.criterion(output, labels)\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect gradients\n",
    "        data_grad = x.grad.data\n",
    "\n",
    "        # Call FGSM Attack, same as fgsm()\n",
    "        sign_data_grad = data_grad.sign()\n",
    "        perturbed_images = x + self.epsilon * sign_data_grad\n",
    "        perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "        return perturbed_images.squeeze()\n",
    "    \n",
    "class ToTensor:\n",
    "    \"\"\"Convert ndarrays in sample to Tensors. Works the same as transforms.ToTensor() but includes labels\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        x, label = sample\n",
    "        return (transforms.functional.to_tensor(x), torch.from_numpy(np.array(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_test(base_path):\n",
    "    \"\"\"given ~two~ ONE model (changed from before), compare how they do under different epsilons of fgsm attack\"\"\"\n",
    "    #testing our trained model\n",
    "    base_model = CNN().to(device)\n",
    "    base_model.load_state_dict(torch.load(base_path))\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    for e in EPSILONS:\n",
    "        transform_fgsm = transforms.Compose([\n",
    "                ToTensor(),\n",
    "                FGSMTransform(epsilon=e) #epsilon\n",
    "            ])\n",
    "        fgsm_test = CustomCIFAR(root='./data', train=False, transform=transform_fgsm)\n",
    "        fgsm_loader = torch.utils.data.DataLoader(dataset=fgsm_test, batch_size=64)\n",
    "        l, a = simple_test(test_loader=fgsm_loader, criterion=criterion, model=base_model)\n",
    "        accuracies.append(a)\n",
    "        losses.append(l)\n",
    "    print(\"\\tAccuracies:\", accuracies)\n",
    "    print(\"\\tLosses:\", losses)\n",
    "    return accuracies, losses\n",
    "    # print(\"Augmented model accuracies:\", accuracies_fgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Training model with epsilon:  0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining model with epsilon: \u001b[39m\u001b[39m\"\u001b[39m, epsilon)\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     20\u001b[0m         model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     21\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/adversarial-networks/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/adversarial-networks/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/adversarial-networks/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/adversarial-networks/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/adversarial-networks/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:240\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mCustomCIFAR.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[39m#CHANGED from the torchvision implementation: pass the target into transform\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform((img, target))\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/Documents/adversarial-networks/venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mFGSMTransform.__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, labels)\n\u001b[1;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     28\u001b[0m \u001b[39m# Collect gradients\u001b[39;00m\n\u001b[1;32m     29\u001b[0m data_grad \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata\n",
      "File \u001b[0;32m~/Documents/adversarial-networks/venv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/adversarial-networks/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epsilon in EPSILONS:\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    fgsm_transform = transforms.Compose([ToTensor(),FGSMTransform(epsilon=epsilon)])\n",
    "\n",
    "    # Download CIFAR10 dataset\n",
    "    train_dataset1 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    train_dataset2 = CustomCIFAR(root='./data', train=True, transform=fgsm_transform)\n",
    "    augmented_dataset = torch.utils.data.ConcatDataset([train_dataset1, train_dataset2])\n",
    "    # Create data loaders\n",
    "    train_loader= torch.utils.data.DataLoader(dataset=augmented_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    model = CNNLSTM().to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    print(\"Training model with epsilon: \", epsilon)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for images, labels in train_loader:\n",
    "            model.train()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # #saving model\n",
    "    name =  str(epsilon)[2:]\n",
    "    torch.save(model.state_dict(), \"./models/cnn-lstm_fgsm\"+name+\".pth\")\n",
    "    #testing model\n",
    "    accuracies[epsilon], losses[epsilon] = compare_test(\"./models/cnn-lstm_fgsm\"+name+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies[0], losses[0] = compare_test(\"./models/cnn.pth\")\n",
    "plt.plot(EPSILONS,accuracies[0], \"-\",label=\"Base model\")\n",
    "\n",
    "#plotting\n",
    "for epsilon in EPSILONS:\n",
    "    plt.plot(EPSILONS,accuracies[epsilon], \"-\",label=\"Trained on ep=\"+str(epsilon))\n",
    "plt.title(\"Epsilon vs Accuracy \")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
