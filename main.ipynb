{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from networks.cnn import CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Download CIFAR10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#create smaller dataset to test with\n",
    "mini_train_idx = torch.utils.data.SubsetRandomSampler(np.arange(200)) # get the first 200 images\n",
    "mini_train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, sampler=mini_train_idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = CNN().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 785/7820 [00:30<04:17, 27.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1569/7820 [01:01<03:41, 28.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 2349/7820 [01:31<03:56, 23.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.8591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 3133/7820 [02:01<02:52, 27.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 1.1446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 3913/7820 [02:31<02:32, 25.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.9419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 4697/7820 [03:02<01:54, 27.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.4461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 5478/7820 [03:32<01:33, 25.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 6259/7820 [04:04<00:58, 26.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.6056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 7043/7820 [04:34<00:28, 26.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7820/7820 [05:04<00:00, 26.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 1.1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7820/7820 [05:20<00:00, 26.26it/s]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "progress = tqdm(total=len(train_loader)*EPOCHS, desc=\"Training\") # add a progress bar\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        model.train()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress.update(1)\n",
    "    \n",
    "    progress.write(f'Epoch [{epoch+1}/{10}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# #saving model\n",
    "# torch.save(model.state_dict(), \"pretrained_cnn.pth\")\n",
    "# print(\"Saved PyTorch Model State to pretrained_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 50062/500000 [02:03<19:01, 394.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 100052/500000 [04:04<15:43, 423.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 150061/500000 [06:01<13:33, 430.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 3.0972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 200052/500000 [07:57<11:44, 426.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 250060/500000 [09:53<09:45, 427.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 2.5820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 300047/500000 [11:49<07:48, 426.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.1338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 350072/500000 [13:45<05:48, 430.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.1622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 400065/500000 [15:42<04:00, 415.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 450078/500000 [17:38<01:55, 431.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 1.1206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500000/500000 [19:34<00:00, 434.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 2.3504\n",
      "Saved PyTorch Model State as cnn_batchsize1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500000/500000 [19:50<00:00, 434.17it/s]"
     ]
    }
   ],
   "source": [
    "#training a model with batchsize=1 to fix dimension errors in the transform of fgsm\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset_bs1 = datasets.CIFAR10(root='./data', train=True, transform=transform)\n",
    "train_loader_bs1 = torch.utils.data.DataLoader(dataset=train_dataset_bs1, batch_size=1, shuffle=True)\n",
    "model_batchsize1 = CNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_batchsize1.parameters(), lr=0.001)\n",
    "EPOCHS = 10\n",
    "# Train the model\n",
    "progress = tqdm(total=len(train_loader_bs1)*EPOCHS, desc=\"Training\") # add a progress bar\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader_bs1:\n",
    "        model_batchsize1.train()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_batchsize1(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress.update(1)\n",
    "    \n",
    "    progress.write(f'Epoch [{epoch+1}/{10}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# #saving model\n",
    "torch.save(model.state_dict(), \"./models/cnn_batchsize1.pth\")\n",
    "print(\"Saved PyTorch Model State as cnn_batchsize1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.2848, Accuracy: 0.0168\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "# model = CNN()\n",
    "# model.load_state_dict(torch.load(\"models/cnn.pth\"))\n",
    "\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        #for every batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = (torch.max(outputs, dim=1)[1] == labels).to(torch.float32).mean()\n",
    "        losses.append(loss.cpu().numpy())\n",
    "        accuracies.append(accuracy.cpu().numpy())\n",
    "\n",
    "loss, accuracy = np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast gradient sign method\n",
    "def fgsm(image, epsilon, data_grad):\n",
    "    # eta = epsilon * sign(gradient of loss w.r.t input image)\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 7322.0 / 10000 = 0.7322\n",
      "Epsilon: 0.001\tTest Accuracy = 7123.0 / 10000 = 0.7123\n",
      "Epsilon: 0.003\tTest Accuracy = 5221.0 / 10000 = 0.5221\n",
      "Epsilon: 0.005\tTest Accuracy = 3132.0 / 10000 = 0.3132\n",
      "Epsilon: 0.007\tTest Accuracy = 1720.0 / 10000 = 0.172\n",
      "Epsilon: 0.1\tTest Accuracy = 0.0 / 10000 = 0.0\n",
      "Epsilon: 0.15\tTest Accuracy = 4.0 / 10000 = 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Adversarial test (but technically also train)\n",
    "\n",
    "epsilons = [0, 0.0005, 0.001, 0.003, 0.007,0.01]\n",
    "pretrained_model = \"models/cnn.pth\"\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the network\n",
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load(pretrained_model))\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def test( model, device, test_loader, epsilon,criterion, optimizer ):\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    for images, labels in test_loader:\n",
    "        # Send the data and label to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor\n",
    "        images.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(images) \n",
    "        init_pred = torch.max(output, dim=1)[1] # get the index of the max log-probability\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect gradients\n",
    "        data_grad = images.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_images = fgsm(images, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_images)\n",
    "\n",
    "        final_pred = torch.max(output, dim=1)[1] # get the index of the max log-probability\n",
    "        correct_idx = (final_pred == labels)\n",
    "        correct += sum(correct_idx.to(torch.float32)).item()\n",
    "\n",
    "        # only get pred that was right buut now wrong\n",
    "        incorrect_idx = (final_pred != labels) & (init_pred == labels)\n",
    "\n",
    "        #saving examples of perturbed images for later visualization\n",
    "        if len(adv_examples) < 5:\n",
    "            # Save some adv examples for visualization later\n",
    "            # p is the single perturbed image, y is the correct label, initial and final and pre- and post-fgsm predictions\n",
    "            for initial, final, p, y in zip(init_pred[incorrect_idx], final_pred[incorrect_idx], perturbed_images[incorrect_idx], labels[incorrect_idx]):\n",
    "                adv_ex = p.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (initial.item(), final.item(), y.item(), adv_ex) )\n",
    "                # returned adv_examples is 1 x batchsize x 4, holding items: pre-fgsm pred, post-fgsm pred, ground truth, post-fgsm image\n",
    "            \n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if epsilon == 0:\n",
    "                for initial, final in zip(init_pred[correct_idx], final_pred[correct_idx]):\n",
    "                    adv_ex = perturbed_images.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (initial.item(), final.item(), final.item(), adv_ex) )\n",
    "\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_dataset))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_dataset), final_acc))\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples\n",
    "\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "epsilons = [0, 0.001, 0.003, 0.005, 0.007, 0.1, 0.15] #overwriting epsilon for faster testing\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps, criterion, optimizer)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print an adversarial example saved during the above program\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "#img needs to be a tensor that is configured with torchvision.utils.make_grid(img), before after and truth are strings\n",
    "def imshow(img, truth, before=\"\", after=\"\"):\n",
    "    npimg = torchvision.utils.make_grid(img).numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(\"Truth: {} Before {} -> After {}\".format(truth, before, after))\n",
    "    plt.show()\n",
    "\n",
    "ex = examples[1][3]\n",
    "img = torch.tensor(ex[3])\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# perturbed image\n",
    "imshow(img, classes[ex[2]], classes[ex[0]], classes[ex[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/isabellaqian/Desktop/ECE C147/adversarial-networks/main.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m mini_idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mSubsetRandomSampler(np\u001b[39m.\u001b[39marange(\u001b[39m200\u001b[39m)) \u001b[39m# get the first 200 images\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m mini_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dataset\u001b[39m=\u001b[39mmini_set, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, sampler\u001b[39m=\u001b[39mmini_idx)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, l \u001b[39min\u001b[39;00m mini_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     imshow(torch\u001b[39m.\u001b[39mtensor(images\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()), \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Users/isabellaqian/Desktop/ECE C147/adversarial-networks/main.ipynb Cell 11\u001b[0m in \u001b[0;36mCustomCIFAR.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m#CHANGED from the torchvision implementation: pass the target into transform\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform((img, target))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# img = self.transform(img)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;32m/Users/isabellaqian/Desktop/ECE C147/adversarial-networks/main.ipynb Cell 11\u001b[0m in \u001b[0;36mFGSMTransform.__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# forward, backward pass to calculate gradient\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m output \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(output, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "#custom class because we need to add the label to the transform\n",
    "from PIL import Image\n",
    "class CustomCIFAR(datasets.CIFAR10):\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "            #CHANGED from the torchvision implementation: pass the target into transform\n",
    "            img = self.transform((img, target))\n",
    "            # img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "    \n",
    "class FGSMTransform:\n",
    "    \"\"\"Perform a fast gradient sign attack on an image.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.005):\n",
    "        self.epsilon = epsilon\n",
    "        self.model = CNN().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.model.load_state_dict(torch.load(\"./models/cnn_batchsize1.pth\"))\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "    def __call__(self,sample):\n",
    "        x, labels = sample\n",
    "        print(x.size())\n",
    "        #if we process a single image instead of batch, we need to add a fourth dimension\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "            print(x.size())\n",
    "        x = x.to(device)\n",
    "        x.requires_grad = True\n",
    "\n",
    "        # forward, backward pass to calculate gradient\n",
    "        output = model(x)\n",
    "        loss = self.criterion(output, labels)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect gradients\n",
    "        data_grad = x.grad.data\n",
    "\n",
    "        # Call FGSM Attack, same as fgsm()\n",
    "        sign_data_grad = data_grad.sign()\n",
    "        perturbed_images = x + self.epsilon * sign_data_grad\n",
    "        perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "        return perturbed_images\n",
    "    \n",
    "class ToTensor:\n",
    "    \"\"\"Convert ndarrays in sample to Tensors, include label\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        x, label = sample\n",
    "        return (transforms.functional.to_tensor(x), torch.from_numpy(np.array(label)))\n",
    "\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "        ToTensor(),\n",
    "        # transforms.Lambda(lambda x: print(x))\n",
    "        FGSMTransform()\n",
    "    ])\n",
    "\n",
    "# mini_set = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "mini_set = CustomCIFAR(root='./data', train=False, transform=transform)\n",
    "mini_idx = torch.utils.data.SubsetRandomSampler(np.arange(200)) # get the first 200 images\n",
    "mini_loader = torch.utils.data.DataLoader(dataset=mini_set, batch_size=64, sampler=mini_idx)\n",
    "for images, l in mini_loader:\n",
    "    imshow(torch.tensor(images.squeeze().detach().cpu().numpy()), \"test\")\n",
    "    break\n",
    "\n",
    "# test_img = torch.tensor(p[0].detach().numpy())\n",
    "# imshow(test_img,\"\")\n",
    "# test_transform = transforms.Compose([\n",
    "#         FGSMTransform()\n",
    "#     ])\n",
    "# x_transformed = test_transform(test_img)\n",
    "# imshow(x_transformed,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this is a shorter function that perturbs a single image. I adapted the FGSM class above from this\n",
    "#note: labels isn't defined in this function (it runs because labels was defined previously)\n",
    "def fgsm_transform(epsilon, x):\n",
    "    # eta = epsilon * sign(gradient of loss w.r.t input image)\n",
    "    # Set requires_grad attribute of tensor\n",
    "    x = x.to(device)\n",
    "    x.requires_grad = True\n",
    "\n",
    "    # forward pass and calculate gradient\n",
    "    output = model(x) \n",
    "    loss = criterion(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Collect gradients\n",
    "    data_grad = x.grad.data\n",
    "\n",
    "    # Call FGSM Attack, same as fgsm()\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_images = x + epsilon*sign_data_grad\n",
    "    perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "    return perturbed_images\n",
    "\n",
    "sample = None\n",
    "labels = None\n",
    "for images, l in mini_train_loader:\n",
    "    if sample == None:\n",
    "        sample = images\n",
    "        labels = l\n",
    "        break\n",
    "p = fgsm_transform(0.005, sample)\n",
    "imshow(torch.tensor(p[0].squeeze().detach().cpu().numpy()), \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
