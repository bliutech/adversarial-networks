{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from networks.cnn import CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Download CIFAR10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#create smaller dataset to test with\n",
    "mini_train_idx = torch.utils.data.SubsetRandomSampler(np.arange(200)) # get the first 200 images\n",
    "mini_train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, sampler=mini_train_idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = CNN().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1570/7820 [4:20:30<17:17:04,  9.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/isabellaqian/Desktop/ECE C147/adversarial-networks/main.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ECE C147/adversarial-networks/networks/cnn.py:21\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Train the model\n",
    "progress = tqdm(total=len(train_loader)*EPOCHS, desc=\"Training\") # add a progress bar\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        model.train()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress.update(1)\n",
    "    \n",
    "    progress.write(f'Epoch [{epoch+1}/{10}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# #saving model\n",
    "# torch.save(model.state_dict(), \"pretrained_cnn.pth\")\n",
    "# print(\"Saved PyTorch Model State to pretrained_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to pretrained_cnn.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#loading the pretrained model\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"pretrained_cnn.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1437, Accuracy: 0.6731\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"pretrained_cnn.pth\"))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        #for every batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = (torch.max(outputs, dim=1)[1] == labels).to(torch.float32).mean()\n",
    "        losses.append(loss.cpu().numpy())\n",
    "        accuracies.append(accuracy.cpu().numpy())\n",
    "\n",
    "loss, accuracy = np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast gradient sign method\n",
    "def fgsm(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    # perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.05\tTest Accuracy = 0.0 / 10000 = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Adversarial test (but technically also train)\n",
    "\n",
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "pretrained_model = \"pretrained_cnn.pth\"\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the network\n",
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load(pretrained_model))\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def test( model, device, test_loader, epsilon,criterion, optimizer ):\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    for images, labels in test_loader:\n",
    "        # Send the data and label to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        images.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(images) \n",
    "        init_pred = torch.max(output, dim=1)[1] # get the index of the max log-probability\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = images.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        #potential error: pass multiple images instead of image??\n",
    "        perturbed_images = fgsm(images, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_images)\n",
    "\n",
    "        final_pred = torch.max(output, dim=1)[1] # get the index of the max log-probability\n",
    "        correct_idx = (final_pred == labels)\n",
    "        correct += sum(correct_idx.to(torch.float32)).item()\n",
    "\n",
    "        # print(correct)\n",
    "        incorrect_idx = (final_pred != labels)\n",
    "\n",
    "        #saving examples of perturbed images for later visualization\n",
    "        if len(adv_examples) < 5:\n",
    "            # Save some adv examples for visualization later\n",
    "            # p is the single perturbed image, y is the correct label, initial and final and pre- and post-fgsm predictions\n",
    "            for initial, final, p, y in zip(init_pred[incorrect_idx], final_pred[incorrect_idx], perturbed_images[incorrect_idx], labels[incorrect_idx]):\n",
    "                adv_ex = p.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (initial.item(), final.item(), y.item(), adv_ex) )\n",
    "                # returned adv_examples is 1 x batchsize x 4\n",
    "            \n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if epsilon == 0:\n",
    "                for initial, final in zip(init_pred[correct_idx], final_pred[correct_idx]):\n",
    "                    adv_ex = perturbed_images.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (initial.item(), final.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_dataset))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_dataset), final_acc))\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples\n",
    "\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "epsilons = [0.05] #overwriting epsilon for faster testing\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps, criterion, optimizer)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#WIP: print the adversarial examples saved during the above program\n",
    "# Plot several examples of adversarial samples at each epsilon\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "def imshow(img, before, after):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(\"Before {} -> After {}\".format(before, after))\n",
    "    plt.show()\n",
    "\n",
    "# print(examples[1][0][2])\n",
    "# images = torch.tensor(examples[1][0][2][0])\n",
    "images = torch.tensor(examples[0][0][3])\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "imshow(torchvision.utils.make_grid(images), classes[examples[0][0][0]], classes[examples[0][0][1]])\n",
    "\n",
    "\n",
    "# cnt = 0\n",
    "\n",
    "# plt.figure()\n",
    "# for i in range(len(epsilons)):\n",
    "#     for j in range(len(examples[i])):\n",
    "#         cnt += 1\n",
    "#         plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "#         plt.xticks([], [])\n",
    "#         plt.yticks([], [])\n",
    "#         if j == 0:\n",
    "#             plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "#         orig,adv,ex = examples[i][j]\n",
    "#         plt.title(\"{} -> {}\".format(orig, adv))\n",
    "#         plt.imshow(ex)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
