{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from networks.cnn import CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Download CIFAR10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#create smaller dataset to test with\n",
    "mini_train_idx = torch.utils.data.SubsetRandomSampler(np.arange(200)) # get the first 200 images\n",
    "mini_train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, sampler=mini_train_idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = CNN().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1570/7820 [4:20:30<17:17:04,  9.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/isabellaqian/Desktop/ECE C147/adversarial-networks/main.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isabellaqian/Desktop/ECE%20C147/adversarial-networks/main.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ECE C147/adversarial-networks/networks/cnn.py:21\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Train the model\n",
    "progress = tqdm(total=len(train_loader)*EPOCHS, desc=\"Training\") # add a progress bar\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        model.train()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress.update(1)\n",
    "    \n",
    "    progress.write(f'Epoch [{epoch+1}/{10}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# #saving model\n",
    "# torch.save(model.state_dict(), \"pretrained_cnn.pth\")\n",
    "# print(\"Saved PyTorch Model State to pretrained_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to pretrained_cnn.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#loading the pretrained model\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"pretrained_cnn.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1437, Accuracy: 0.6731\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"pretrained_cnn.pth\"))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        #for every batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = (torch.max(outputs, dim=1)[1] == labels).to(torch.float32).mean()\n",
    "        losses.append(loss.cpu().numpy())\n",
    "        accuracies.append(accuracy.cpu().numpy())\n",
    "\n",
    "loss, accuracy = np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast gradient sign method\n",
    "def fgsm(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 6952.0 / 10000 = 0.6952\n",
      "Epsilon: 0.001\tTest Accuracy = 6525.0 / 10000 = 0.6525\n",
      "Epsilon: 0.003\tTest Accuracy = 4031.0 / 10000 = 0.4031\n",
      "Epsilon: 0.005\tTest Accuracy = 2012.0 / 10000 = 0.2012\n",
      "Epsilon: 0.007\tTest Accuracy = 913.0 / 10000 = 0.0913\n",
      "Epsilon: 0.1\tTest Accuracy = 0.0 / 10000 = 0.0\n",
      "Epsilon: 0.15\tTest Accuracy = 0.0 / 10000 = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Adversarial test (but technically also train)\n",
    "\n",
    "epsilons = [0, 0.0005, 0.001, 0.003, 0.007,0.01]\n",
    "pretrained_model = \"pretrained_cnn.pth\"\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the network\n",
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load(pretrained_model))\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def test( model, device, test_loader, epsilon,criterion, optimizer ):\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    for images, labels in test_loader:\n",
    "        # Send the data and label to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        images.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(images) \n",
    "        init_pred = torch.max(output, dim=1)[1] # get the index of the max log-probability\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect gradients\n",
    "        data_grad = images.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_images = fgsm(images, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_images)\n",
    "\n",
    "        final_pred = torch.max(output, dim=1)[1] # get the index of the max log-probability\n",
    "        correct_idx = (final_pred == labels)\n",
    "        correct += sum(correct_idx.to(torch.float32)).item()\n",
    "\n",
    "        # only get pred that was right buut now wrong\n",
    "        incorrect_idx = (final_pred != labels) & (init_pred == labels)\n",
    "\n",
    "        #saving examples of perturbed images for later visualization\n",
    "        if len(adv_examples) < 5:\n",
    "            # Save some adv examples for visualization later\n",
    "            # p is the single perturbed image, y is the correct label, initial and final and pre- and post-fgsm predictions\n",
    "            for initial, final, p, y in zip(init_pred[incorrect_idx], final_pred[incorrect_idx], perturbed_images[incorrect_idx], labels[incorrect_idx]):\n",
    "                adv_ex = p.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (initial.item(), final.item(), y.item(), adv_ex) )\n",
    "                # returned adv_examples is 1 x batchsize x 4, holding items: pre-fgsm pred, post-fgsm pred, ground truth, post-fgsm image\n",
    "            \n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if epsilon == 0:\n",
    "                for initial, final in zip(init_pred[correct_idx], final_pred[correct_idx]):\n",
    "                    adv_ex = perturbed_images.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (initial.item(), final.item(), final.item(), adv_ex) )\n",
    "\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_dataset))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_dataset), final_acc))\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples\n",
    "\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "epsilons = [0, 0.001, 0.003, 0.005, 0.007, 0.1, 0.15] #overwriting epsilon for faster testing\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps, criterion, optimizer)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print the adversarial examples saved during the above program\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "#img needs to be a tensor that is configured with torchvision.utils.make_grid(img), before after and truth are strings\n",
    "def imshow(img, before, after, truth):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(\"Truth: {} Before {} -> After {}\".format(truth, before, after))\n",
    "    plt.show()\n",
    "\n",
    "ex = examples[1][0]\n",
    "img = torch.tensor(ex[3])\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# perturbed image\n",
    "imshow(torchvision.utils.make_grid(img), classes[ex[0]], classes[ex[1]], classes[ex[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
